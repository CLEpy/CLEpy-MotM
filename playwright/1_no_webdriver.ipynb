{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://quotes.toscrape.com/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting all quotes on a page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request web page (HTML) \n",
    "r = requests.get(\"https://quotes.toscrape.com/\")\n",
    "# Load page text into a Beautiful Soup object\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "# get all elements with quote blocks\n",
    "quote_elements = soup.find_all(name=\"span\", attrs={\"class\": \"text\"})\n",
    "# Get only text for each element\n",
    "quotes = [i.text for i in quote_elements]\n",
    "# First three quotes\n",
    "quotes[:3], len(quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get quotes from every page (10 pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_quotes = []\n",
    "next_page = 1\n",
    "while next_page:\n",
    "    print(next_page, end=\" \")\n",
    "    # add page number to URL, then request web page (HTML)\n",
    "    if next_page == 1:\n",
    "        r = requests.get(BASE_URL)\n",
    "    else:\n",
    "        r = requests.get(f\"{BASE_URL}page/{next_page}/\")\n",
    "    # Create Beautiful soup object from HTML \n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    # get all elements with quote blocks\n",
    "    quote_elements = soup.find_all(name=\"span\", attrs={\"class\": \"text\"})\n",
    "    # Get only text for each element\n",
    "    quotes = [i.text for i in quote_elements]\n",
    "    # add that page's quotes to the big list\n",
    "    all_quotes.extend(quotes)\n",
    "    # Find next page\n",
    "    next_page_link = soup.find(\"li\", {\"class\": \"next\"})\n",
    "    # if next page isn't found (None) then we're on the last page, break loop\n",
    "    if not next_page_link:\n",
    "        break\n",
    "    next_page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_quotes[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_quotes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cc34502990468a9a043a86aebbfd509c61f8b91091a7d7c337becedf5550fd66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
