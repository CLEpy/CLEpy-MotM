{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy in a jupyter notebook\n",
    "#### What is Scrapy?\n",
    "<img src = \"scrapy_diagram.png\" width = 700 align = \"left\"></img><br>\n",
    "- <b>Engine: </b>Orchestration of the data flow between all components of the framework and triggering events when certain actions occur.\n",
    "- <b>Spider: </b>What we define, how we tell scrapy which parts of the website to gather information from (structured data)\n",
    "- <b>Scheduler: </b>handles concurrency, throughput, and other policies\n",
    "- <b>Downloader: </b>gets the url and passes back to engine\n",
    "- <b>Pipeline: </b> Where the data retrieved by the downloader and processed by the Spider goes. Can be processed further, validated, or stored (persistence).\n",
    "\n",
    "\n",
    "Source: https://www.jitsejan.com/using-scrapy-in-jupyter-notebook.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Settings for notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Show Python version\n",
    "import platform\n",
    "platform.python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import scrapy\n",
    "except:\n",
    "    !pip install scrapy\n",
    "    import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up pipeline\n",
    "This class creates a simple pipeline that writes all found items to a JSON file, where each line contains one JSON element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JsonWriterPipeline(object):\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('cplresult.jl', 'w')\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CplSpider(scrapy.Spider):\n",
    "    name = \"cpl\"\n",
    "    start_urls = [\n",
    "        'https://cpl.org/board-agendas/'\n",
    "    ]\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, # Used for pipeline 1\n",
    "        'FEED_FORMAT':'json',                                 # Used for pipeline 2\n",
    "        'FEED_URI': 'cplresult.json'                        # Used for pipeline 2\n",
    "    }\n",
    "    \n",
    "    def _parse_classification(self, title):\n",
    "        if \"committee\" in title.lower():\n",
    "            return \"COMMITTEE\"\n",
    "        if \"commission\" in title.lower():\n",
    "            return \"COMMISSION\"\n",
    "        return \"BOARD\"\n",
    "    \n",
    "    def _parse_location(self, response):\n",
    "        elems = response.css('.has-text-align-center::text').extract()\n",
    "        if \"abundance\" in elems[0].lower() and \"zoom\" not in elems[0].lower():\n",
    "            meeting_name = response.css('.has-text-align-center::text').extract()[0]+\"Zoom\"\n",
    "        else:\n",
    "            meeting_name = response.css('.has-text-align-center::text').extract()[0]\n",
    "        if \"zoom\" not in meeting_name.lower():\n",
    "            address = \"17109 Lake Shore Blvd Cleveland, OH 44110\"\n",
    "        else:\n",
    "            address = response.css('.has-text-align-center').css('a::attr(href)').extract()\n",
    "            if len(address)<2:\n",
    "                add = [x for x in response.css('.has-text-align-center::text').extract() if \"meeting id\" in x.lower()][0]\n",
    "                _phone = add.split(\"(\")[0]\n",
    "                _meeting_id = add.split(\"(\")[1]\n",
    "                \n",
    "                meeting_id = [x for x in _meeting_id if x.isnumeric()]\n",
    "                zoom_link = \"https://cpl.zoom.us/j/\" + \"\".join(meeting_id)\n",
    "                phone = f\"tel:{''.join([x for x in _phone if x.isnumeric()])}\"\n",
    "                if phone not in address and len(phone)>len(\"tel:\"):\n",
    "                    address.append(phone) \n",
    "                if zoom_link not in address and len(zoom_link)>len(\"https://cpl.zoom.us/j/\"):\n",
    "                    address.append(zoom_link) \n",
    "        return {\"name\": name, \"address\": address}\n",
    "    \n",
    "    \n",
    "    def _parse_links(self, response):\n",
    "        result = {}\n",
    "        for text, link in zip(response.css(\".entry-content\").xpath(\"ol\").css(\"a::text\").extract(), \n",
    "                              response.css(\".entry-content\").xpath(\"ol\").css(\"a::attr(href)\").extract()):\n",
    "            result[text]=link\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def _parse_times(self, title, summary):\n",
    "        \"\"\"may not be accurate AM/PM\"\"\"\n",
    "        date_match = re.search(r\"[a-zA-Z]{3,10} \\d{1,2},? \\d{4}\", title)\n",
    "        time = re.findall(r\"(\\d{1,2}:\\d{2})\", summary)\n",
    "        return datetime.strptime(\" \".join([date_match.group(0), time]), \"%B %d, %Y %I:%M\")\n",
    "    \n",
    "    def _parse_meetings(self, response):\n",
    "        title = response.css('.entry-title::text').get()\n",
    "        times = response.css('.has-text-align-center::text').extract()[3]\n",
    "        location = response.css('.has-text-align-center::text').extract()[0]+\"Zoom.\"\n",
    "        yield {\"title\": title.replace(\"Agenda\",\"\").strip(),\n",
    "               #\"time\": str(self._parse_times(title, times)),\n",
    "               \"classification\": self._parse_classification(title),\n",
    "               \"location\": self._parse_location(response),\n",
    "               \"links\": self._parse_links(response)\n",
    "              }\n",
    "    \n",
    "    def parse(self, response):\n",
    "        #A Response object represents an HTTP response, which is usually downloaded (by the Downloader) \n",
    "        # and fed to the Spiders for processing.\n",
    "        for meeting_link in response.xpath('//h2[@class=\"entry-title\"]//@href').getall():\n",
    "            yield scrapy.Request(\n",
    "                meeting_link,\n",
    "                callback = self._parse_meetings,\n",
    "                dont_filter = True\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-12 17:39:07 [scrapy.utils.log] INFO: Scrapy 2.4.1 started (bot: scrapybot)\n",
      "2021-04-12 17:39:07 [scrapy.utils.log] INFO: Versions: lxml 4.5.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.8.3 (default, Jul  2 2020, 11:26:31) - [Clang 10.0.0 ], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 2.9.2, Platform macOS-10.15.7-x86_64-i386-64bit\n",
      "2021-04-12 17:39:07 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2021-04-12 17:39:07 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 30,\n",
      " 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2021-04-12 17:39:07 [py.warnings] WARNING: /Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/extensions/feedexport.py:247: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\n",
      "  exporter = cls(crawler)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Deferred at 0x7fb4be031dc0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-12 17:39:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://cpl.org/board-agendas/january-21-2021-board-of-trustees-2021-organizational-meeting-agenda/> (referer: https://cpl.org/board-agendas/)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/utils/defer.py\", line 120, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/utils/python.py\", line 353, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/utils/python.py\", line 353, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py\", line 29, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py\", line 340, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"<ipython-input-7-e88d8a1d08b3>\", line 64, in _parse_meetings\n",
      "    \"time\": str(self._parse_times(title, times)),\n",
      "  File \"<ipython-input-7-e88d8a1d08b3>\", line 57, in _parse_times\n",
      "    return datetime.strptime(\" \".join([date_match.group(0), time]), \"%B %d, %Y %I:%M\")\n",
      "TypeError: sequence item 1: expected str instance, list found\n",
      "2021-04-12 17:39:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://cpl.org/board-agendas/february-16-2021-board-of-trustees-regular-meeting-agenda/> (referer: https://cpl.org/board-agendas/)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/utils/defer.py\", line 120, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/utils/python.py\", line 353, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/utils/python.py\", line 353, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py\", line 29, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py\", line 340, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"<ipython-input-7-e88d8a1d08b3>\", line 64, in _parse_meetings\n",
      "    \"time\": str(self._parse_times(title, times)),\n",
      "  File \"<ipython-input-7-e88d8a1d08b3>\", line 57, in _parse_times\n",
      "    return datetime.strptime(\" \".join([date_match.group(0), time]), \"%B %d, %Y %I:%M\")\n",
      "TypeError: sequence item 1: expected str instance, list found\n",
      "2021-04-12 17:39:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://cpl.org/board-agendas/march-18-2021-board-of-trustees-regular-meeting-agenda/> (referer: https://cpl.org/board-agendas/)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/utils/defer.py\", line 120, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/utils/python.py\", line 353, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/utils/python.py\", line 353, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py\", line 29, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py\", line 340, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"<ipython-input-7-e88d8a1d08b3>\", line 64, in _parse_meetings\n",
      "    \"time\": str(self._parse_times(title, times)),\n",
      "  File \"<ipython-input-7-e88d8a1d08b3>\", line 57, in _parse_times\n",
      "    return datetime.strptime(\" \".join([date_match.group(0), time]), \"%B %d, %Y %I:%M\")\n",
      "TypeError: sequence item 1: expected str instance, list found\n",
      "2021-04-12 17:39:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://cpl.org/board-agendas/march-16-2021-joint-finance-human-resources-committee-meeting-agenda/> (referer: https://cpl.org/board-agendas/)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/utils/defer.py\", line 120, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/utils/python.py\", line 353, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/utils/python.py\", line 353, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py\", line 29, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py\", line 340, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"<ipython-input-7-e88d8a1d08b3>\", line 64, in _parse_meetings\n",
      "    \"time\": str(self._parse_times(title, times)),\n",
      "  File \"<ipython-input-7-e88d8a1d08b3>\", line 57, in _parse_times\n",
      "    return datetime.strptime(\" \".join([date_match.group(0), time]), \"%B %d, %Y %I:%M\")\n",
      "TypeError: sequence item 1: expected str instance, list found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-12 17:39:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://cpl.org/board-agendas/april-13-2021-joint-finance-human-resources-committee-meeting-agenda/> (referer: https://cpl.org/board-agendas/)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/utils/defer.py\", line 120, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/utils/python.py\", line 353, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/utils/python.py\", line 353, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py\", line 29, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py\", line 340, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"<ipython-input-7-e88d8a1d08b3>\", line 64, in _parse_meetings\n",
      "    \"time\": str(self._parse_times(title, times)),\n",
      "  File \"<ipython-input-7-e88d8a1d08b3>\", line 57, in _parse_times\n",
      "    return datetime.strptime(\" \".join([date_match.group(0), time]), \"%B %d, %Y %I:%M\")\n",
      "TypeError: sequence item 1: expected str instance, list found\n",
      "2021-04-12 17:39:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://cpl.org/board-agendas/january-21-2021-board-of-trustees-regular-meeting/> (referer: https://cpl.org/board-agendas/)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/utils/defer.py\", line 120, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/utils/python.py\", line 353, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/utils/python.py\", line 353, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py\", line 29, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py\", line 340, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/Users/nmolivo/anaconda3/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"<ipython-input-7-e88d8a1d08b3>\", line 64, in _parse_meetings\n",
      "    \"time\": str(self._parse_times(title, times)),\n",
      "  File \"<ipython-input-7-e88d8a1d08b3>\", line 57, in _parse_times\n",
      "    return datetime.strptime(\" \".join([date_match.group(0), time]), \"%B %d, %Y %I:%M\")\n",
      "TypeError: sequence item 1: expected str instance, list found\n"
     ]
    }
   ],
   "source": [
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "\n",
    "process.crawl(CplSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "city-scrapers-cle",
   "language": "python",
   "name": "city-scrapers-cle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
